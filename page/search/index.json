[{"content":"As a programmer today, it is likely you will find yourself working in a polyglot, or multiple language environment. We have finally gotten to the place where the merits of different programming technologies can coexist peacefully together. We all know developers are an opinionated breed of individuals. I certainly find a comfort level with specific technology stacks over others. That is completely ok by the way, don\u0026rsquo;t let anyone tell you that your preferred stack is somehow any worse than a different one! You may find strengths/weaknesses across programming languages for different use cases, however having valid evidence to support those claims is way better than simply stating an opinion as fact. Regardless, what is often not okay, is rebuilding functionality in a given language with similar or identical requirements when it doesn\u0026rsquo;t make a measurable difference to do so. After all, we are not providing business value to our customers (at least directly) which is often our primary objective. Someone could argue certainly that it provides developer benefit, maybe it even makes us more efficient as developers, but often with tight deadlines and difficult objectives to hit, we have to make tradeoffs in the work that the team is able to accomplish for a given project, sprint, etc. This brings us to the topic of today\u0026rsquo;s post. My development shop has started adopting the AWS CDK as our primary Infrastructure as Code (IaC) solution for greenfield development. I am personally all for this as domain specific languages like HCL for Terraform can feel clunky and cumbersome to utilize with a mindset geared towards for-loops, if conditionals, functions and all the other \u0026ldquo;goodies\u0026rdquo; we find in traditional programming languages. In addition, we get strong bundling and dependency management capabilities that help the CDK apps we write feel like any other app we are used to coding! ### end preview ### There are efficiencies to gain here, as we can now build custom reusable constructs (think libraries) in the CDK to spin up common infrastructure patterns quickly and easily! We can use a custom construct to do the \u0026ldquo;heavy-lifting\u0026rdquo; which allows us to focus on writing business logic and delivering value! We can build packages codifying business requirements for our infrastructure like: common tagging models, use of specific custom domains, security best practices and so much more! Thus, we increase our velocity and deliver more for our customers. However, as previously alluded to, not everyone loves a given programming language. On the web side at our company we currently do a lot of .NET development and our embedded group might gravitate towards something like Python. Again, different strokes for different folks. Differences in opinion are ok, but if we want to achieve economies of scale and get the maximum amount of reuse from these custom constructs, we need a way to meet both groups in the middle. Enter JSii, a fantastic offering from AWS that enables developers to write constructs one time using TypeScript to support all the programming languages CDK targets today. So with one code base we can support C#, Python, Go, Java, JavaScript and of course TypeScript. This embraces our polyglot environment and ensures we are using our time efficiently. Everyone walks away happy able to use their favorite programming language. So how does it work?\nJSii sounds complicated on the surface, after all generating code into multiple languages is certainly not an easy problem to solve, but with the great tooling built by the AWS team we can largely be \u0026ldquo;ignorant\u0026rdquo; to those implementation details provided you adhere to using TypeScript as the base language and follow a few rules for the TypeScript being written. Let\u0026rsquo;s walk through a simple example construct to spin up everyone\u0026rsquo;s favorite demo infrastructure an S3 bucket. Note you should have the CDK installed on your shell to begin, instructions can be found here. Start by creating a new CDK construct project inside a directory named \u0026ldquo;MyAwesomeBucket\u0026rdquo; using the following command:\n1 cdk init lib --language=typescript This will create a new CDK construct project with a construct name and an associated properties interface matching the directory name. Inside the MyAwesomeBucket/lib directory you will find index.ts. This is your starter class inheriting from the standard CDK Construct class. Here we can define our custom construct and any configuration properties it might need. Replace the MyAwesomeBucket class definition with the following code and remove the MyAwesomeBucketProps interface as it won\u0026rsquo;t be needed for this exercise:\n1 2 3 4 5 6 7 8 9 10 11 export class MyAwesomeBucket extends Construct { constructor(scope: Construct, id: string) { super(scope, id); new s3.Bucket(scope, \u0026#39;MyAwesomeBucket\u0026#39;, { encryption: s3.BucketEncryption.S3_MANAGED, enforceSSL: true }); } } Here we have a simple construct that creates an S3 bucket with encryption and a bucket policy that will enforce SSL. These are great best practices that maybe we would want to enforce in our infrastructure across the company. With JSii we can take this code and publish it in a language specific registry for our desired targets. The entire organization can now use it, getting all those best practices for \u0026ldquo;free\u0026rdquo;. There are only two properties and one resource in this example, but this tiny construct shows the power a larger construct can have in speeding up development and codifying best practices throughout the organization. After all, we can simply new up an instance of this class in our CDK infrastructure stack (after installing the dependency) to create in this case a bucket with encryption and SSL protection already enabled!\nSo with that established, how do we generate constructs using the JSii tooling for all these language targets? First we should install JSii dependencies into our package.json. Run the following command in your solution:\n1 npm install jsii jsii-pacmak -D This command installs the two tools we need for this simple walkthrough. JSii will provide the TypeScript compiler for our construct and jsii-pacmak will provide the tooling to generate language-specific packages from our construct code. Speaking of which, JSii will generate your tsconfig.json file for you, so go ahead and delete that file. The next step requires us to make some updates to our package.json. First replace the script block with the following JSii specific commands:\n1 2 3 4 5 \u0026#34;scripts\u0026#34;: { \u0026#34;build\u0026#34;: \u0026#34;jsii -vvv\u0026#34;, \u0026#34;watch\u0026#34;: \u0026#34;jsii -w\u0026#34;, \u0026#34;package\u0026#34;: \u0026#34;jsii-pacmak -vvv\u0026#34; } In addition, we must add an author object property with name and email specified, the repository object property with the url set to the git repository location, a valid license property, a description and a homepage are also recommended suggestions. While these items seem trivial, the JSii tooling is very particular about how the package.json file looks. Finally, we must add a jsii object property similar to the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026#34;jsii\u0026#34;: { \u0026#34;outdir\u0026#34;: \u0026#34;dist\u0026#34;, \u0026#34;targets\u0026#34;: { \u0026#34;python\u0026#34;: { \u0026#34;distName\u0026#34;: \u0026#34;MyAwesomeBucket\u0026#34;, \u0026#34;module\u0026#34;: \u0026#34;MyAwesomeBucket\u0026#34; }, \u0026#34;dotnet\u0026#34;: { \u0026#34;namespace\u0026#34;: \u0026#34;MyAwesomeBucketNS\u0026#34;, \u0026#34;packageId\u0026#34;: \u0026#34;MyAwesomeBucket\u0026#34; } } } This property tells the JSii tooling where to place the compiled (ready-to-publish) packages and what language targets you want to support. To keep things concise, we will use just Python and .NET. Simply specify your naming configuration for each target and JSii will handle the rest! Now we should be able to run the following command and verify there are no errors (note the watch command is handy to use during development as it watches for changes and runs the build on save, enhancing the feedback loop while working on the code):\n1 npm run build Assuming there were no errors we are finally ready to build the packages. As you can probably guess running the package command will do the deed. However, you may find yourself on a development machine without specific versions of tooling for your configured targets. No worries, JSii has you covered with this Docker image jsii/superchain! Choose the image version tag that fits your needs and mount a volume or clone your repository inside the docker image. A command like this one should help get you there if you choose to mount a volume:\n1 docker run -it --rm --entrypoint sh -v /dev/temp/MyAwesomeBucket:/MyAwesomeBucket jsii/superchain:latest Inside the container, change your directory into the MyAwesomeBucket folder, run your dependency installation if necessary and finally run the following:\n1 npm run build \u0026amp;\u0026amp; npm run package Assuming everything is successful your JSii outdir (dist if you followed the snippet above) will now have a folder for each target with artifacts ready for publishing to npm, PyPi and Nuget with standard publishing tools! So I know what you are likely thinking, this is great but what is the catch? There are always tradeoffs in these situations where we sacrifice something for the flexibility and benefits a tool like JSii offers. Well honestly the only thing I have found to date are performance implications. As this post details, the \u0026ldquo;magic\u0026rdquo; behind JSii is that native language calls proxy JavaScript code to an embedded JavaScript VM, enabling the write once, run everywhere benefits we love. Obviously, this has implications and might rule it out for some performance sensitive use cases. If you read further in the post there are some benchmark comparisons for you to examine. I would argue for my company\u0026rsquo;s needs, we are simply running this infrastructure code in CICD to spin up/update AWS architecture. If it takes a little longer to do so, that is not a deal breaker. After all, it is not like this infrastructure code is hosting web traffic that needs to happen quickly and at scale!\nIn closing, JSii brings a ton of value to the table and can satisfy everyone by meeting them at the level of the tools and languages they use daily. Languages are likely to be added over time and with AWS backing/supporting the JSii ecosystem, you can relax in knowing it will be maintained for years to come. I personally am hoping to see an inner-sourcing initiative take off at my company with everyone contributing, consuming and using these packages to ultimately move faster and drive business value.\n","date":"2022-03-14T00:00:00Z","permalink":"https://shawn.vause.us/posts/jsii-for-cdk/","title":"JSii for Polyglot Shops Writing AWS CDK Constructs"},{"content":"AWS Lambda supports a variety of ways to get code and dependencies up and running in the cloud. You can use Lambda layers, custom runtimes or for ultimate control just specify a full container definition and push that up to ECR/Docker Hub for use by the Lambda platform. However, as flexible as Lambda is, there are two approaches that stand above the rest as \u0026ldquo;drop-dead simple\u0026rdquo;. You can write your code in the browser with the built in editor, but let\u0026rsquo;s be honest that isn\u0026rsquo;t the best development experience, or you can zip your assets up locally and push them to the cloud. This is of course provided that your assets/dependencies are less than the 50MB cap at the time of writing this post. The remaining content of this post will focus on the zip deployment approach and problems you can run into when working on a Mac/Windows environment which most professional developers are likely to spend the bulk of their time in. ### end preview ### Python being the target function language for our routine has some design considerations to orient ourselves with when we do a ​pip install​ of package dependencies locally. Typically, a pip package will be compiled on the platform it is installed on by the python wheel. As a result, you can run into problems where MacOS does things differently than Windows or Linux during that compilation. For example, MacOS handles encryption differently than Windows and differently than Linux being a FreeBSD based operating system. The compiled dependency will likely be incompatible with the Linux OS running in the Lambda platform. Any zip file we push up to the cloud must have the dependencies compiled for the host Linux context!\nSo how do we solve the previously described problem? Enter the lambci collection of docker images which includes the ​lambci/lambda:build-python3.8 for python version 3.8. This image includes all the necessary build tooling for installing python dependencies with pip. The goal of the lambci project is to produce a docker image that is as close to the AWS host environment for a Lambda function as possible. This is a great way to test your function code locally while iterating quickly during development. Using the build image previously mentioned, we are able to build our zip file within docker for uploading to AWS and share it out locally via a docker volume. Here is a handy script you can use in bash for creating your python code archive:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/bin/sh # Execute from your python3.8 code directory # Copy contents to temp directory cp -r ./ ./temp \u0026amp;\u0026amp; cd ./temp # Run the lambci container with a volume in the personal working directory. Then execute the commands # in the container to do a local pip install. Finally, zip contents from the /var/task path in the container. docker run -v \u0026#34;$PWD\u0026#34;:/var/task lambci/lambda:build-python3.8 cd /var/task \u0026amp;\u0026amp; \\ pip install --target ./packages -r requirements.txt \u0026amp;\u0026amp; \\ zip -r mycode.zip ./ -x \u0026#34;packages/*\u0026#34; \u0026amp;\u0026amp; \\ cd packages/ \u0026amp;\u0026amp; \\ zip -ur ../mycode.zip ./ The comments explain what is going on, but it is important to mention that when we create the zip file we initially exclude the packages directory created by the targeted pip install​. We then add the installed packages to the root of the zip avoiding folder nesting that will prevent the dependencies from being recognized by the Lambda runtime. Take a look at this documentation to upload your zip into AWS Lambda via the AWS CLI! If all goes according to plan you will be running your function with properly detected dependencies!\n","date":"2021-12-03T00:00:00Z","permalink":"https://shawn.vause.us/posts/python-local-to-lambda/","title":"Getting Python to AWS Lambda From Non-Linux Environments"},{"content":"Pulumi is a powerful infrastructure as code (IaC) tool with a fantastic superpower you just don\u0026rsquo;t get with products like HashiCorp\u0026rsquo;s Terraform. You can harness the power of your favorite programming languages and apply them for another purpose. Rather than learning a domain specific language (DSL) that is largely a \u0026ldquo;one-trick pony\u0026rdquo; you can make your favorite programming language build cloud infrastructure! How cool is that? HashiCorp Configuration Language (HCL), while powerful, is frankly used for one thing and one thing only, infrastructure. It can do the job well, there is no argument we have gotten our mileage out of this tool and tools like it (I am looking at you AWS Cloudformation). However, it immediately starts to break down as you get more sophisticated. Looping, conditionals, etc. while supported have largely been bolted on as customers started to hit the edges of this tool chain. Which begged the important and maybe obvious question, \u0026ldquo;Why are we forcing ourselves to use a language not designed for this level of sophistication, when we have world class languages that would certainly be up to the task?\u0026rdquo; As a result, I would argue another wave of IaC tools was kicked off with players like AWS launching their Cloud Development Kit (CDK), Pulumi launching a cross-cloud/cross-service SDK and even HashiCorp recognized the need to grow past HCL. I will leave the reader to investigate the merits of each toolchain, as this was not the intent of this post. Today I want to introduce the Pulumi stack concept and the flexibility it can provide in building our cloud infrastructure. ### end preview ### Let\u0026rsquo;s start by setting the stage. Pulumi is organized per their documentation as follows: Based on this diagram we see that a project is essentially a collection of program(s) that create resources producing inputs and outputs. What is not immediately clear and is an issue I have with this diagram are the stacks. While a project certainly contains programs, resources and stacks, it isn\u0026rsquo;t apparent that stacks are essentially an isolated \u0026ldquo;snapshot\u0026rdquo; of a program\u0026rsquo;s execution and the resources it produced. They map very well to system environment contexts (as shown with dev, qa and prod callouts). In other words, stacks, put simply, define the state of our infrastructure for that \u0026ldquo;snapshot\u0026rdquo; under an identifier like the environment name.\nWith the aforementioned said, another point of friction is with the documentation around projects. The documentation seems to overload the \u0026ldquo;project\u0026rdquo; label as it is represented on this diagram. It seems to refer to \u0026ldquo;projects\u0026rdquo; not only as a collection of programs, but also as a program itself. They clarify this further by explaining that any directory containing the ​Pulumi.yml file is considered a \u0026ldquo;project\u0026rdquo;. This unfortunately introduces some confusion for the reader. When you look at the UI in the Pulumi state management solution you would expect the root element of the Pulumi hierarchy to be the name codified in the yaml file. However it is not, there is a higher root level concept that maps to your repository (for example in GitHub). For sake of discussion, we will refer to this GitHub project as a \u0026ldquo;group\u0026rdquo; since it appears to organize Pulumi projects (programs) with their stacks. This results in my opinion of what the mental model should actually look like for Pulumi (for the sake of clarity program details which remain unchanged were left off this new diagram version): Clearing up these confusion points results in a better understanding when talking about project boundaries. When examining these boundaries, you might make the assumption that these boxes are isolated contexts and thus each project deploys independently from each other. While possibly true, this isn\u0026rsquo;t a hard and fast requirement. Imagine a situation where you have static assets that don\u0026rsquo;t change often. Perhaps you have an S3 bucket that stores image files for a website. The bucket might get used in multiple places (multiple websites) and thus lends itself to grouping under a shared infrastructure project. This grouping not only organizes resources by intent, but it can also help us prevent accidental destruction of these assets. If they were stored with more ephemeral resources, like servers that have a higher tendency of being torn down and spun up fresh, then there is inherently more risk that something could go wrong or would be destroyed unintentionally. In addition, splitting these projects up and sharing outputs from the stacks gives us incredible flexibility to segregate our resources by purpose, enabling us to adhere to single responsibility principals we know and love. The Pulumi documentation acknowledges that newcomers are likely to start with a monolithic stack (i.e.: everything in one project deploying via a single stack). While this works fine in some situations, the real power of these abstractions comes from the interplay across project boundaries and stacks. You can also reduce execution time by controlling which projects you run ​pulumi up​ on during a given deployment.\nLet\u0026rsquo;s walk through an example. This is a simple two project stack utilizing DigitalOcean Droplets (servers, think EC2) and DigitalOcean Spaces (object storage, think S3). In our first stack, we have a simple DigitalOcean Spaces bucket. In our second stack, we have a simple server hosting a blog application that allows users to post content. This content as you would likely suspect includes images that we want to store in our spaces bucket. DigitalOcean Spaces gives us a free CDN on top of their object storage solution, so it sounds like a good way to keep that load off our server and force the browser to pull that content from the bucket. I will be using C# to illustrate this example, however it is important to mention the APIs are very similar across languages:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 public class SharedStack : Stack { [Output]​ public Output BucketName { get; set; } [Output] public Output CdnEndpoint { get; set; } ​ public SharedStack() { var bucket = new Pulumi.DigitalOcean.SpacesBucket(\u0026#34;my-bucket\u0026#34;, new Pulumi.DigitalOcean.SpacesBucketArgs { Region = \u0026#34;nyc3\u0026#34;, Acl = \u0026#34;private\u0026#34;​ ​ }); var cdn = new Pulumi.DigitalOcean.Cdn(\u0026#34;my-cdn\u0026#34;, new Pulumi.DigitalOcean.CdnArgs​ { Origin = bucket.BucketDomainName​ });​ ​ // Outputs​ BucketName = bucket.Name; CdnEndpoint = cdn.Endpoint;​​ }​ } We can then import the outputs using a StackReference in our server stack!\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 public class BlogStack : Stack { ​ public BlogStack() { var sharedStack = new Pulumi.StackReference(\u0026#34;organization/project/environment\u0026#34;);​ var bucketName = sharedStack.RequireOutput(\u0026#34;BucketName\u0026#34;); var cdnEndpoint = sharedStack.RequireOutput(\u0026#34;CdnEndpoint\u0026#34;); var userData = Output.All(bucketName, cdnEndpoint) .Apply(t =\u0026gt; { var userData = \u0026#34;#!/bin/bash\u0026#34;; // Set environment variables userData += $\u0026#34;\\necho \\\u0026#34;Bucket=\\\\\\\u0026#34;{t[0]}\\\\\\\u0026#34;\\\u0026#34; | sudo tee -a /etc/environment\u0026#34;; userData += $\u0026#34;\\necho \\\u0026#34;CDN=\\\\\\\u0026#34;https://{t[1]}/\\\\\\\u0026#34;\\\u0026#34; | sudo tee -a /etc/environment\u0026#34;; return Output.CreateSecret(userData); }); var webServer = new Pulumi.DigitalOcean.Droplet(\u0026#34;my-server\u0026#34;, new Pulumi.DigitalOcean.DropletArgs { Image = \u0026#34;docker-20-04\u0026#34;, Size = \u0026#34;s-1vcpu-1gb\u0026#34;, VpcUuid = vpcId, PrivateNetworking = true, Region = \u0026#34;nyc3\u0026#34;, SshKeys = { sshKeyId }, UserData = userData }); }​ }​ There is a ton of power here and we are only stratching the surface. The big takeaways are that you shouldn\u0026rsquo;t be afraid of embracing multiple projects and stacks. Each stack is deployed via pulumi up independently, allowing you to focus on deploying only the infrastructure needed at a given time, reducing risk. It also enables a shared stack of resources you can use throughout your cloud solutions.\n","date":"2021-09-27T00:00:00Z","permalink":"https://shawn.vause.us/posts/think-outside-stack-pulumi/","title":"Think Outside the Stack With Pulumi"},{"content":"You may have noticed the blog went down recently. I have been working on a new design and re-launch that took a touch longer than anticipated. Thank you for returning. I am hoping to have a more robust solution available to write/publish content more easily now. ### end preview ### I intend to use this space to document things I have learned in my technical journeys and personal reading. Occasionally something non-technical might slip in, as I have used the pandemic to pursue new hobbies like fishing. While I may not be the most avid blogger, I think it will be a great reference for stuff I need to refer to periodically and my hope is that it will provide value to others as well. Thanks again for returning and I look forward to writing more content/iterating on this platform. Please bear with me for any bugs/defects that might arise as this is definitely a v1.0 platform at the moment. Welcome back and cheers! ","date":"2021-09-09T00:00:00Z","permalink":"https://shawn.vause.us/posts/welcome/","title":"Welcome Back!"}]