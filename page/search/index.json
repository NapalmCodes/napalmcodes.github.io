[{"content":"Observability of solutions has become a critical component of modern distributed applications that deserves more forethought and consideration than often afforded to it. Typically, implementing visibility to application functionality is relegated to a last minute implementation detail for the sole purpose of checking a box to indicate the system has been delivered with core expectations. While I can understand developers are strapped with a never-ending backlog of features to deliver (often features with percieved higher value to stakeholders) there is frankly no feature of higher value for development teams and those supporting their applications than observability. When you are awakened in the middle of the night from your slumber to solve a system problem, you want to see where the issue is quickly and with the \u0026ldquo;right\u0026rdquo; level of detail to indicate what the solution is likely to be. Not only is this beneficial to restoring functionality for your customers, but let\u0026rsquo;s be honest, we need to get our asses back to bed quickly to handle the upcoming day. Enter OpenTelemetry, a tool to make our jobs easier and get the visibilty we need quickly, effectively and with a relatively easy implementation process.  OpenTelemetry (OTEL) is rapidly becoming the defacto solution for collecting observability data from our applications in the form of traces, metrics and logs (primer available here). A key benefit of using OTEL, is that it is vendor agnostic and thus enables us to self-host our observability stack or choose from multiple vendors that support OTEL telemetry. By standardizing how we instrument our applications we can move faster and get intelligent insights that correlate events in complex distributed systems built with a variety of different technologies and programming languages. Luckily, OTEL\u0026rsquo;s rise in popularity has lead to most modern languages providing an SDK to instrument across heterogenous distributed solutions. This post focuses on .NET based solutions, but be aware of the robust offerings available here. OTEL helps with the collection process of telemetry and getting it out of our way quickly and easily, so we can focus on instrumenting our apps and providing value for ourselves and support teams from the start of a development project.\nSetup For this post, we are going to instrument a simple Web API solution using ASP.NET Core. Create a new ASP.NET core Web API project utilizing .NET 8 and install the following nuget packages (at time of writing this post):\nPackage Name Version Description OpenTelemetry 1.7.0 Core OTEL Functionality. OpenTelemetry.Exporter.Console 1.7.0 Basic data exporter displaying telemetry in stdout. OpenTelemetry.Exporter.OpenTelemetryProtocol 1.7.0 Data exporter using OTEL Protocol to transmit observability data (to OTEL Collector). OpenTelemetry.Extensions.Hosting 1.7.0 Extensions to utilize OTEL in the Microsoft Hosting environment. OpenTelemetry.Instrumentation.AspNetCore 1.7.1 Enables collection of ASP.NET Core observability data already built into it's libraries. OpenTelemetry.Instrumentation.Http 1.7.1 Enables collection of HTTPClient observability data (again already built into the libraries). OpenTelemetry.Instrumentation.Runtime 1.7.0 This is a cool one, get all the interesting observability data (allocations, garbage collections, etc.) from the dotnet runtime itself! Implementation I am not sure if this is considered official best practice, but anytime I add a lot of \u0026ldquo;noise\u0026rdquo; to the dependency injection system, I always create a class(es) of extension methods to encapsulate core feature sets or group similar \u0026ldquo;services\u0026rdquo;. This helps keep the registration pipeline clean and readable. Create the following class as follows and we will step through it:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 using OpenTelemetry.Instrumentation.AspNetCore; using OpenTelemetry.Logs; using OpenTelemetry.Metrics; using OpenTelemetry.Resources; using OpenTelemetry.Trace; using System.Diagnostics.Metrics; namespace WebApplication1; public static class OpenTelemetryExtensions { private static readonly Action\u0026lt;ResourceBuilder\u0026gt; _ConfigureResource = r =\u0026gt; r.AddService( serviceName: \u0026#34;web-application-1\u0026#34;, serviceVersion: typeof(Program).Assembly.GetName() .Version?.ToString() ?? \u0026#34;unknown\u0026#34;, serviceInstanceId: Environment.MachineName); public static IServiceCollection AddOpenTelemetryObservability( this IServiceCollection services, IConfiguration configuration) { services.AddOpenTelemetry() .ConfigureResource(_ConfigureResource) .WithTracing(builder =\u0026gt; { builder.AddSource() .SetSampler(new AlwaysOnSampler()) .AddHttpClientInstrumentation() .AddAspNetCoreInstrumentation(); services.Configure\u0026lt;AspNetCoreTraceInstrumentationOptions\u0026gt;( configuration.GetSection(\u0026#34;AspNetCoreInstrumentation\u0026#34;)); builder.SetupTracingExporter(configuration); }) .WithMetrics(builder =\u0026gt; { builder // Used to record/export specific types // of metrics (counters, gagues, etc.) //.AddMeter() .AddRuntimeInstrumentation() .AddHttpClientInstrumentation() .AddAspNetCoreInstrumentation(); builder.SetupMetricsView(configuration); builder.SetupMetricsExporter(configuration); }); return services; } public static void AddOpenTelemetryLogging(this ILoggingBuilder builder, IConfiguration configuration) { var logExporter = configuration.GetValue( \u0026#34;UseLogExporter\u0026#34;, defaultValue: \u0026#34;console\u0026#34;)!.ToLowerInvariant(); builder.ClearProviders(); builder.AddOpenTelemetry(options =\u0026gt; { var resourceBuilder = ResourceBuilder.CreateDefault(); _ConfigureResource(resourceBuilder); options.SetResourceBuilder(resourceBuilder); options.SetupLogsExporter(logExporter, configuration); }); } private static void SetupTracingExporter( this TracerProviderBuilder builder, IConfiguration configuration) { var tracingExporter = configuration.GetValue( \u0026#34;UseTracingExporter\u0026#34;, defaultValue: \u0026#34;console\u0026#34;)!.ToLowerInvariant(); switch (tracingExporter) { case \u0026#34;otlp\u0026#34;: builder.AddOtlpExporter(otlpOptions =\u0026gt; { otlpOptions.Endpoint = new Uri( configuration.GetValue( \u0026#34;Otlp:Endpoint\u0026#34;, defaultValue: \u0026#34; http://localhost:4317\u0026#34;)!); }); break; default: builder.AddConsoleExporter(); break; } } private static void SetupMetricsExporter( this MeterProviderBuilder builder, IConfiguration configuration) { var metricsExporter = configuration.GetValue(\u0026#34;UseMetricsExporter\u0026#34;, defaultValue: \u0026#34;console\u0026#34;)! .ToLowerInvariant(); switch (metricsExporter) { case \u0026#34;otlp\u0026#34;: builder.AddOtlpExporter(otlpOptions =\u0026gt; { otlpOptions.Endpoint = new Uri( configuration.GetValue( \u0026#34;Otlp:Endpoint\u0026#34;, defaultValue: \u0026#34; http://localhost:4317\u0026#34;)!); }); break; default: builder.AddConsoleExporter(); break; } } private static void SetupMetricsView( this MeterProviderBuilder builder, IConfiguration configuration) { var histogramAggregation = configuration.GetValue( \u0026#34;HistogramAggregation\u0026#34;, defaultValue: \u0026#34;explicit\u0026#34;)! .ToLowerInvariant(); switch (histogramAggregation) { case \u0026#34;exponential\u0026#34;: builder.AddView(instrument =\u0026gt; { return instrument.GetType() .GetGenericTypeDefinition() == typeof(Histogram\u0026lt;\u0026gt;) ? new Base2ExponentialBucketHistogramConfiguration() : null; }); break; default: // Explicit bounds histogram is default, nothing to do. break; } } private static void SetupLogsExporter( this OpenTelemetryLoggerOptions options, string logExporter, IConfiguration configuration) { switch (logExporter) { case \u0026#34;otlp\u0026#34;: options.AddOtlpExporter(otlpOptions =\u0026gt; { otlpOptions.Endpoint = new Uri( configuration.GetValue( \u0026#34;Otlp:Endpoint\u0026#34;, defaultValue: \u0026#34; http://localhost:4317\u0026#34;)!); }); break; default: options.AddConsoleExporter(); break; } } } Configuration The code is pretty readable in my opinion, but essentially we are using a configuration file with the following structure to drive the appropriate OTEL behaviors:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \u0026#34;Logging\u0026#34;: { \u0026#34;LogLevel\u0026#34;: { \u0026#34;Default\u0026#34;: \u0026#34;Information\u0026#34;, \u0026#34;Microsoft.AspNetCore\u0026#34;: \u0026#34;Warning\u0026#34; } }, \u0026#34;UseTracingExporter\u0026#34;: \u0026#34;otlp\u0026#34;, \u0026#34;UseMetricsExporter\u0026#34;: \u0026#34;otlp\u0026#34;, \u0026#34;UseLogExporter\u0026#34;: \u0026#34;otlp\u0026#34;, \u0026#34;HistogramAggregation\u0026#34;: \u0026#34;explicit\u0026#34;, \u0026#34;AspNetCoreInstrumentation\u0026#34;: { \u0026#34;RecordException\u0026#34;: \u0026#34;true\u0026#34; }, \u0026#34;Otlp\u0026#34;: { \u0026#34;Endpoint\u0026#34;: \u0026#34;http://localhost:4317\u0026#34; }, \u0026#34;AllowedHosts\u0026#34;: \u0026#34;*\u0026#34; } For each \u0026ldquo;Use\u0026lt;Data Type\u0026gt;Exporter\u0026rdquo; property we take two string values currently \u0026ldquo;console\u0026rdquo; or \u0026ldquo;otlp\u0026rdquo;. This configures the system to use either a console exporter (great for testing) or the OpenTelemetry Protocol which is a vendor/tool agnostic protocol for transmitting traces, metrics and logs telemetry to another service. We will use something called the OTEL Collector as this service, but be aware that logging tools like DataDog and others support OTLP protocol natively in some cases. In addition, we provide an OTLP endpoint for sending data to the OTEL collector from our API. The last parameter controls how the histogram buckets metric data (explicit boundaries vs exponential scales).\nCode Explanation There are two public extension methods in the class defined above. One controls setting up the traces and metrics collection process for OTEL and the other (while similar) sets up the application logging provider. Using the settings from the appsettings.json file we add appropriate exporters for getting observability data points out of our API. You will also notice that we add instrumentation for the various .NET core functionality that is made available to us (HTTPClient, Dotnet Runtime, AspNetCore) for trace and metrics. This is incredibly powerful by itself as a ton of telemetry for our solution is already built-in just waiting for us to export it somewhere we can make use of it!\nAnother important callout, is the use of the ConfigureResource action method. OTEL will use the value configured as a service name tag value. This is really useful for grouping telemetry in an environment where maybe there are many distributed services working independently/interactively. It also allows us to tag our telemetry with a software version (think about handling blue/green deployments and rolling updates where multiple version of a service are live at the same time) and a machine name where the data originated from (load balanced environments).\nHook Up To utilize the extension methods above it is easy to wire them into the HTTP builder pipeline for your web application. In Program.cs:\n1 2 3 4 5 6 7 var builder = WebApplication.CreateBuilder(args); // Add this line below the above builder assignment to add OpenTelemetry for tracing and metrics. builder.Services.AddOpenTelemetryObservability(builder.Configuration); // Add this line to add OpenTelemetry for logging. builder.Logging.AddOpenTelemetryLogging(builder.Configuration); I would recommend starting with the console exporter and verifying that you start to see OTEL yaml for logs and metrics/trace data being written out to the console window.\nIn my next post, I will discuss taking this data from the console and pushing it to the OpenTelemetry Collector for display in Grafana! We will also touch on adding custom instrumentation to your code!\n","date":"2024-02-24T00:00:00Z","permalink":"https://shawn.vause.us/posts/opentelemetry-dotnet/","title":"OpenTelemetry for .NET Solutions with Grafana Tooling"},{"content":"Contrary to popular belief, it is possible to run an iOS application on your iPhone/iPad without paying the $99 USD fee per year to Apple for a developer program account. Through a process called free provisioning we can gain this freedom. It is a relatively simple task to accomplish and only involves some clicking around Xcode. There is no jailbreaking involved and it is supported by Apple. It does have some limitations that we will get into, but for those learning mobile development in Apple\u0026rsquo;s walled garden of a platform, it is a great way to get started without the added expense.\nWe will demonstrate how free provisioning gets us started running code on our iOS device, through the lens of the relatively new .NET MAUI platform. For those unaware, .NET MAUI is a solution that allows us to write a single UI using XAML or Blazor based controls that will run across iOS, Android, Mac and Windows. There are also capabilities to call into platform specific code as needs dictate, for instance to access sensors or platform specific APIs like those provided for in-app purchases. A simulator does exist for iOS on the Mac and Visual Studio provides some great tooling around remoting to your simulator from Windows environments. However, there is no substitute for running your software on physical hardware while developing. Frankly, it is a faster experience and a lot closer to how your customers will use your application.  Requirements Visual Studio 2022 17.3.0 or above with the .NET MAUI workload installed A Mac to compile and plug your iPhone/iPad device into Xcode latest version installed on your Mac An Apple ID not connected to Apple Developer Program Xcode Steps Begin by opening Xcode on your Mac and navigating via the menu bar to Xcode \u0026gt; Preferences. Under the Accounts tab, click the plus button to add your Apple ID account to Xcode using your user name and password. Close the dialog and proceed to create a new App project. Make sure to note your exact bundle identifier (in my case com.maui.FreeProv) and choose the team \u0026ldquo;Your Name (Personal Team)\u0026rdquo; created when you added your Apple ID to Xcode.\nOnce Xcode is done doing its thing, plug your iOS device into the Mac and unlock your phone so you can confirm you trust the mac to access your device, example below:\nYou will also need to enable Developer Mode on your phone by visiting Settings \u0026gt; Privacy and Security \u0026gt; Developer Mode and enabling the setting with the toggle. Note, your device will need to restart at this point and you must confirm that you want to turn developer mode on after the phone is unlocked.\nBack in Xcode, at the top of your App project\u0026rsquo;s window, you will find a bread crumb control beside the project name. It is mostly likely pointing at an iPhone (model) value. Click that and choose your device in the dropdown under the iOS Device heading.\nNext we have to configure the project settings to get our bundle identity signed. To do this click on the left side of the Xcode window in the project navigator on the top level node, in my case the FreeProv project. This will open the project properties tab. Verify the bundle identifier from earlier (remember I had you note this value) it must be exactly the same as the value you saw when creating the project. This is an important time to call out the profile we create is only for this identifier. When we setup some configuration in Visual Studio it will have to match this value exactly yet again. Set the Display Name while we are working in this section. Another value to note is the Minimum Deployments setting. This value must match or be lower than the iOS version running on the device you are using.\nFinally, switch tabs in the project settings dialog to Signing \u0026amp; Capabilities and choose to Automatically manage signing with the Your Name (Personal Team) value selected.\nIf you click on the information icon next to the words Xcode Managed Profile you will see what was generated. You will also see the biggest limitation of this free provisioning process. The provisioning profile is only good for six days! Thankfully, you can just renew this value using an abbreviated version of the process discussed thus far. Please note though, signing identities will expire after a year.\nAt this point you can optionally deploy the sample application from Xcode to your device if you want to verify it is working as expected. Simply click the run button. It probably failed at this point saying it was an untrusted developer who built the app. Once again visit Settings and go to General \u0026gt; VPN \u0026amp; Device Management. There will be a developer app setting with your email there it says \u0026ldquo;Not Trusted\u0026rdquo; underneath. Click it and press the Trust \u0026ldquo;Apple Development: (your email) button. Finally, confirm in the dialog that you trust yourself (unless you really don\u0026rsquo;t \u0026#x1f60f;). Run the app again and things should go more smoothly this time.\nVisual Studio Steps Leave your iOS device connected to the Mac. Connect to your Mac from Visual Studio. Instructions can be found here if needed. On your new MAUI project (I did the .NET MAUI Blazor App for fun) navigate to your project properties in the solution explorer. Scroll down to the iOS settings block on the left hand side and click Bundle Signing. Here you need to ensure the scheme is set to Manual Provisioning and signing identity is Developer (Automatic).\nFinally, open up the Info.plist file via your solution explorer and ensure the Bundle Identifier is set to the exact value I had you note before. This effectively links the VS project to the profile created on the connected Mac.\nWith a little luck, you should be able to choose your device from the iOS Remote Device menu of the Visual Studio run tool. This will compile the iOS app and ship it to your device for launch.\nConclusion To summarize, it is possible for devs getting started to run their code on an iOS device without paying the yearly \u0026ldquo;Apple tax\u0026rdquo;. However, it is not the most intuitive process and only buys you a small window of time while developing before the process needs to be repeated. Most application services will not be available with free provisioning as well including Apple Pay, iCloud and In-App Purchasing just to name a few. While disappointing, at least this gives potential iOS developers a chance to experiment before committing to an Apple project with their wallets.\n","date":"2022-10-04T00:00:00Z","permalink":"https://shawn.vause.us/posts/maui-free-ios-provisioning/","title":".NET MAUI iOS Free Provisioning"},{"content":"As a programmer today, it is likely you will find yourself working in a polyglot, or multiple language environment. We have finally gotten to the place where the merits of different programming technologies can coexist peacefully together. We all know developers are an opinionated breed of individuals. I certainly find a comfort level with specific technology stacks over others. That is completely ok by the way, don\u0026rsquo;t let anyone tell you that your preferred stack is somehow any worse than a different one! You may find strengths/weaknesses across programming languages for different use cases, however having valid evidence to support those claims is way better than simply stating an opinion as fact. Regardless, what is often not okay, is rebuilding functionality in a given language with similar or identical requirements when it doesn\u0026rsquo;t make a measurable difference to do so. After all, we are not providing business value to our customers (at least directly) which is often our primary objective. Someone could argue certainly that it provides developer benefit, maybe it even makes us more efficient as developers, but often with tight deadlines and difficult objectives to hit, we have to make tradeoffs in the work that the team is able to accomplish for a given project, sprint, etc. This brings us to the topic of today\u0026rsquo;s post. My development shop has started adopting the AWS CDK as our primary Infrastructure as Code (IaC) solution for greenfield development. I am personally all for this as domain specific languages like HCL for Terraform can feel clunky and cumbersome to utilize with a mindset geared towards for-loops, if conditionals, functions and all the other \u0026ldquo;goodies\u0026rdquo; we find in traditional programming languages. In addition, we get strong bundling and dependency management capabilities that help the CDK apps we write feel like any other app we are used to coding!  There are efficiencies to gain here, as we can now build custom reusable constructs (think libraries) in the CDK to spin up common infrastructure patterns quickly and easily! We can use a custom construct to do the \u0026ldquo;heavy-lifting\u0026rdquo; which allows us to focus on writing business logic and delivering value! We can build packages codifying business requirements for our infrastructure like: common tagging models, use of specific custom domains, security best practices and so much more! Thus, we increase our velocity and deliver more for our customers. However, as previously alluded to, not everyone loves a given programming language. On the web side at our company we currently do a lot of .NET development and our embedded group might gravitate towards something like Python. Again, different strokes for different folks. Differences in opinion are ok, but if we want to achieve economies of scale and get the maximum amount of reuse from these custom constructs, we need a way to meet both groups in the middle. Enter JSii, a fantastic offering from AWS that enables developers to write constructs one time using TypeScript to support all the programming languages CDK targets today. So with one code base we can support C#, Python, Go, Java, JavaScript and of course TypeScript. This embraces our polyglot environment and ensures we are using our time efficiently. Everyone walks away happy able to use their favorite programming language. So how does it work?\nJSii sounds complicated on the surface, after all generating code into multiple languages is certainly not an easy problem to solve, but with the great tooling built by the AWS team we can largely be \u0026ldquo;ignorant\u0026rdquo; to those implementation details provided you adhere to using TypeScript as the base language and follow a few rules for the TypeScript being written. Let\u0026rsquo;s walk through a simple example construct to spin up everyone\u0026rsquo;s favorite demo infrastructure an S3 bucket. Note you should have the CDK installed on your shell to begin, instructions can be found here. Start by creating a new CDK construct project inside a directory named \u0026ldquo;MyAwesomeBucket\u0026rdquo; using the following command:\n1 cdk init lib --language=typescript This will create a new CDK construct project with a construct name and an associated properties interface matching the directory name. Inside the MyAwesomeBucket/lib directory you will find index.ts. This is your starter class inheriting from the standard CDK Construct class. Here we can define our custom construct and any configuration properties it might need. Replace the MyAwesomeBucket class definition with the following code and remove the MyAwesomeBucketProps interface as it won\u0026rsquo;t be needed for this exercise:\n1 2 3 4 5 6 7 8 9 10 11 export class MyAwesomeBucket extends Construct { constructor(scope: Construct, id: string) { super(scope, id); new s3.Bucket(scope, \u0026#39;MyAwesomeBucket\u0026#39;, { encryption: s3.BucketEncryption.S3_MANAGED, enforceSSL: true }); } } Here we have a simple construct that creates an S3 bucket with encryption and a bucket policy that will enforce SSL. These are great best practices that maybe we would want to enforce in our infrastructure across the company. With JSii we can take this code and publish it in a language specific registry for our desired targets. The entire organization can now use it, getting all those best practices for \u0026ldquo;free\u0026rdquo;. There are only two properties and one resource in this example, but this tiny construct shows the power a larger construct can have in speeding up development and codifying best practices throughout the organization. After all, we can simply new up an instance of this class in our CDK infrastructure stack (after installing the dependency) to create in this case a bucket with encryption and SSL protection already enabled!\nSo with that established, how do we generate constructs using the JSii tooling for all these language targets? First we should install JSii dependencies into our package.json. Run the following command in your solution:\n1 npm install jsii jsii-pacmak -D This command installs the two tools we need for this simple walkthrough. JSii will provide the TypeScript compiler for our construct and jsii-pacmak will provide the tooling to generate language-specific packages from our construct code. Speaking of which, JSii will generate your tsconfig.json file for you, so go ahead and delete that file. The next step requires us to make some updates to our package.json. First replace the script block with the following JSii specific commands:\n1 2 3 4 5 \u0026#34;scripts\u0026#34;: { \u0026#34;build\u0026#34;: \u0026#34;jsii -vvv\u0026#34;, \u0026#34;watch\u0026#34;: \u0026#34;jsii -w\u0026#34;, \u0026#34;package\u0026#34;: \u0026#34;jsii-pacmak -vvv\u0026#34; } In addition, we must add an author object property with name and email specified, the repository object property with the url set to the git repository location, a valid license property, a description and a homepage are also recommended suggestions. While these items seem trivial, the JSii tooling is very particular about how the package.json file looks. Finally, we must add a jsii object property similar to the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026#34;jsii\u0026#34;: { \u0026#34;outdir\u0026#34;: \u0026#34;dist\u0026#34;, \u0026#34;targets\u0026#34;: { \u0026#34;python\u0026#34;: { \u0026#34;distName\u0026#34;: \u0026#34;MyAwesomeBucket\u0026#34;, \u0026#34;module\u0026#34;: \u0026#34;MyAwesomeBucket\u0026#34; }, \u0026#34;dotnet\u0026#34;: { \u0026#34;namespace\u0026#34;: \u0026#34;MyAwesomeBucketNS\u0026#34;, \u0026#34;packageId\u0026#34;: \u0026#34;MyAwesomeBucket\u0026#34; } } } This property tells the JSii tooling where to place the compiled (ready-to-publish) packages and what language targets you want to support. To keep things concise, we will use just Python and .NET. Simply specify your naming configuration for each target and JSii will handle the rest! Now we should be able to run the following command and verify there are no errors (note the watch command is handy to use during development as it watches for changes and runs the build on save, enhancing the feedback loop while working on the code):\n1 npm run build Assuming there were no errors we are finally ready to build the packages. As you can probably guess running the package command will do the deed. However, you may find yourself on a development machine without specific versions of tooling for your configured targets. No worries, JSii has you covered with this Docker image jsii/superchain! Choose the image version tag that fits your needs and mount a volume or clone your repository inside the docker image. A command like this one should help get you there if you choose to mount a volume:\n1 docker run -it --rm --entrypoint sh -v /dev/temp/MyAwesomeBucket:/MyAwesomeBucket jsii/superchain:latest Inside the container, change your directory into the MyAwesomeBucket folder, run your dependency installation if necessary and finally run the following:\n1 npm run build \u0026amp;\u0026amp; npm run package Assuming everything is successful your JSii outdir (dist if you followed the snippet above) will now have a folder for each target with artifacts ready for publishing to npm, PyPi and Nuget with standard publishing tools! So I know what you are likely thinking, this is great but what is the catch? There are always tradeoffs in these situations where we sacrifice something for the flexibility and benefits a tool like JSii offers. Well honestly the only thing I have found to date are performance implications. As this post details, the \u0026ldquo;magic\u0026rdquo; behind JSii is that native language calls proxy JavaScript code to an embedded JavaScript VM, enabling the write once, run everywhere benefits we love. Obviously, this has implications and might rule it out for some performance sensitive use cases. If you read further in the post there are some benchmark comparisons for you to examine. I would argue for my company\u0026rsquo;s needs, we are simply running this infrastructure code in CICD to spin up/update AWS architecture. If it takes a little longer to do so, that is not a deal breaker. After all, it is not like this infrastructure code is hosting web traffic that needs to happen quickly and at scale!\nIn closing, JSii brings a ton of value to the table and can satisfy everyone by meeting them at the level of the tools and languages they use daily. Languages are likely to be added over time and with AWS backing/supporting the JSii ecosystem, you can relax in knowing it will be maintained for years to come. I personally am hoping to see an inner-sourcing initiative take off at my company with everyone contributing, consuming and using these packages to ultimately move faster and drive business value.\n","date":"2022-03-14T00:00:00Z","permalink":"https://shawn.vause.us/posts/jsii-for-cdk/","title":"JSii for Polyglot Shops Writing AWS CDK Constructs"},{"content":"AWS Lambda supports a variety of ways to get code and dependencies up and running in the cloud. You can use Lambda layers, custom runtimes or for ultimate control just specify a full container definition and push that up to ECR/Docker Hub for use by the Lambda platform. However, as flexible as Lambda is, there are two approaches that stand above the rest as \u0026ldquo;drop-dead simple\u0026rdquo;. You can write your code in the browser with the built in editor, but let\u0026rsquo;s be honest that isn\u0026rsquo;t the best development experience, or you can zip your assets up locally and push them to the cloud. This is of course provided that your assets/dependencies are less than the 50MB cap at the time of writing this post. The remaining content of this post will focus on the zip deployment approach and problems you can run into when working on a Mac/Windows environment which most professional developers are likely to spend the bulk of their time in.  Python being the target function language for our routine has some design considerations to orient ourselves with when we do a ​pip install​ of package dependencies locally. Typically, a pip package will be compiled on the platform it is installed on by the python wheel. As a result, you can run into problems where MacOS does things differently than Windows or Linux during that compilation. For example, MacOS handles encryption differently than Windows and differently than Linux being a FreeBSD based operating system. The compiled dependency will likely be incompatible with the Linux OS running in the Lambda platform. Any zip file we push up to the cloud must have the dependencies compiled for the host Linux context!\nSo how do we solve the previously described problem? Enter the lambci collection of docker images which includes the ​lambci/lambda:build-python3.8 for python version 3.8. This image includes all the necessary build tooling for installing python dependencies with pip. The goal of the lambci project is to produce a docker image that is as close to the AWS host environment for a Lambda function as possible. This is a great way to test your function code locally while iterating quickly during development. Using the build image previously mentioned, we are able to build our zip file within docker for uploading to AWS and share it out locally via a docker volume. Here is a handy script you can use in bash for creating your python code archive:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/bin/sh # Execute from your python3.8 code directory # Copy contents to temp directory cp -r ./ ./temp \u0026amp;\u0026amp; cd ./temp # Run the lambci container with a volume in the personal working directory. Then execute the commands # in the container to do a local pip install. Finally, zip contents from the /var/task path in the container. docker run -v \u0026#34;$PWD\u0026#34;:/var/task lambci/lambda:build-python3.8 cd /var/task \u0026amp;\u0026amp; \\ pip install --target ./packages -r requirements.txt \u0026amp;\u0026amp; \\ zip -r mycode.zip ./ -x \u0026#34;packages/*\u0026#34; \u0026amp;\u0026amp; \\ cd packages/ \u0026amp;\u0026amp; \\ zip -ur ../mycode.zip ./ The comments explain what is going on, but it is important to mention that when we create the zip file we initially exclude the packages directory created by the targeted pip install​. We then add the installed packages to the root of the zip avoiding folder nesting that will prevent the dependencies from being recognized by the Lambda runtime. Take a look at this documentation to upload your zip into AWS Lambda via the AWS CLI! If all goes according to plan you will be running your function with properly detected dependencies!\n","date":"2021-12-03T00:00:00Z","permalink":"https://shawn.vause.us/posts/python-local-to-lambda/","title":"Getting Python to AWS Lambda From Non-Linux Environments"},{"content":"Pulumi is a powerful infrastructure as code (IaC) tool with a fantastic superpower you just don\u0026rsquo;t get with products like HashiCorp\u0026rsquo;s Terraform. You can harness the power of your favorite programming languages and apply them for another purpose. Rather than learning a domain specific language (DSL) that is largely a \u0026ldquo;one-trick pony\u0026rdquo; you can make your favorite programming language build cloud infrastructure! How cool is that? HashiCorp Configuration Language (HCL), while powerful, is frankly used for one thing and one thing only, infrastructure. It can do the job well, there is no argument we have gotten our mileage out of this tool and tools like it (I am looking at you AWS Cloudformation). However, it immediately starts to break down as you get more sophisticated. Looping, conditionals, etc. while supported have largely been bolted on as customers started to hit the edges of this tool chain. Which begged the important and maybe obvious question, \u0026ldquo;Why are we forcing ourselves to use a language not designed for this level of sophistication, when we have world class languages that would certainly be up to the task?\u0026rdquo; As a result, I would argue another wave of IaC tools was kicked off with players like AWS launching their Cloud Development Kit (CDK), Pulumi launching a cross-cloud/cross-service SDK and even HashiCorp recognized the need to grow past HCL. I will leave the reader to investigate the merits of each toolchain, as this was not the intent of this post. Today I want to introduce the Pulumi stack concept and the flexibility it can provide in building our cloud infrastructure.  Let\u0026rsquo;s start by setting the stage. Pulumi is organized per their documentation as follows: Based on this diagram we see that a project is essentially a collection of program(s) that create resources producing inputs and outputs. What is not immediately clear and is an issue I have with this diagram are the stacks. While a project certainly contains programs, resources and stacks, it isn\u0026rsquo;t apparent that stacks are essentially an isolated \u0026ldquo;snapshot\u0026rdquo; of a program\u0026rsquo;s execution and the resources it produced. They map very well to system environment contexts (as shown with dev, qa and prod callouts). In other words, stacks, put simply, define the state of our infrastructure for that \u0026ldquo;snapshot\u0026rdquo; under an identifier like the environment name.\nWith the aforementioned said, another point of friction is with the documentation around projects. The documentation seems to overload the \u0026ldquo;project\u0026rdquo; label as it is represented on this diagram. It seems to refer to \u0026ldquo;projects\u0026rdquo; not only as a collection of programs, but also as a program itself. They clarify this further by explaining that any directory containing the ​Pulumi.yml file is considered a \u0026ldquo;project\u0026rdquo;. This unfortunately introduces some confusion for the reader. When you look at the UI in the Pulumi state management solution you would expect the root element of the Pulumi hierarchy to be the name codified in the yaml file. However it is not, there is a higher root level concept that maps to your repository (for example in GitHub). For sake of discussion, we will refer to this GitHub project as a \u0026ldquo;group\u0026rdquo; since it appears to organize Pulumi projects (programs) with their stacks. This results in my opinion of what the mental model should actually look like for Pulumi (for the sake of clarity program details which remain unchanged were left off this new diagram version): Clearing up these confusion points results in a better understanding when talking about project boundaries. When examining these boundaries, you might make the assumption that these boxes are isolated contexts and thus each project deploys independently from each other. While possibly true, this isn\u0026rsquo;t a hard and fast requirement. Imagine a situation where you have static assets that don\u0026rsquo;t change often. Perhaps you have an S3 bucket that stores image files for a website. The bucket might get used in multiple places (multiple websites) and thus lends itself to grouping under a shared infrastructure project. This grouping not only organizes resources by intent, but it can also help us prevent accidental destruction of these assets. If they were stored with more ephemeral resources, like servers that have a higher tendency of being torn down and spun up fresh, then there is inherently more risk that something could go wrong or would be destroyed unintentionally. In addition, splitting these projects up and sharing outputs from the stacks gives us incredible flexibility to segregate our resources by purpose, enabling us to adhere to single responsibility principals we know and love. The Pulumi documentation acknowledges that newcomers are likely to start with a monolithic stack (i.e.: everything in one project deploying via a single stack). While this works fine in some situations, the real power of these abstractions comes from the interplay across project boundaries and stacks. You can also reduce execution time by controlling which projects you run ​pulumi up​ on during a given deployment.\nLet\u0026rsquo;s walk through an example. This is a simple two project stack utilizing DigitalOcean Droplets (servers, think EC2) and DigitalOcean Spaces (object storage, think S3). In our first stack, we have a simple DigitalOcean Spaces bucket. In our second stack, we have a simple server hosting a blog application that allows users to post content. This content as you would likely suspect includes images that we want to store in our spaces bucket. DigitalOcean Spaces gives us a free CDN on top of their object storage solution, so it sounds like a good way to keep that load off our server and force the browser to pull that content from the bucket. I will be using C# to illustrate this example, however it is important to mention the APIs are very similar across languages:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 public class SharedStack : Stack { [Output]​ public Output BucketName { get; set; } [Output] public Output CdnEndpoint { get; set; } ​ public SharedStack() { var bucket = new Pulumi.DigitalOcean.SpacesBucket(\u0026#34;my-bucket\u0026#34;, new Pulumi.DigitalOcean.SpacesBucketArgs { Region = \u0026#34;nyc3\u0026#34;, Acl = \u0026#34;private\u0026#34;​ ​ }); var cdn = new Pulumi.DigitalOcean.Cdn(\u0026#34;my-cdn\u0026#34;, new Pulumi.DigitalOcean.CdnArgs​ { Origin = bucket.BucketDomainName​ });​ ​ // Outputs​ BucketName = bucket.Name; CdnEndpoint = cdn.Endpoint;​​ }​ } We can then import the outputs using a StackReference in our server stack!\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 public class BlogStack : Stack { ​ public BlogStack() { var sharedStack = new Pulumi.StackReference(\u0026#34;organization/project/environment\u0026#34;);​ var bucketName = sharedStack.RequireOutput(\u0026#34;BucketName\u0026#34;); var cdnEndpoint = sharedStack.RequireOutput(\u0026#34;CdnEndpoint\u0026#34;); var userData = Output.All(bucketName, cdnEndpoint) .Apply(t =\u0026gt; { var userData = \u0026#34;#!/bin/bash\u0026#34;; // Set environment variables userData += $\u0026#34;\\necho \\\u0026#34;Bucket=\\\\\\\u0026#34;{t[0]}\\\\\\\u0026#34;\\\u0026#34; | sudo tee -a /etc/environment\u0026#34;; userData += $\u0026#34;\\necho \\\u0026#34;CDN=\\\\\\\u0026#34;https://{t[1]}/\\\\\\\u0026#34;\\\u0026#34; | sudo tee -a /etc/environment\u0026#34;; return Output.CreateSecret(userData); }); var webServer = new Pulumi.DigitalOcean.Droplet(\u0026#34;my-server\u0026#34;, new Pulumi.DigitalOcean.DropletArgs { Image = \u0026#34;docker-20-04\u0026#34;, Size = \u0026#34;s-1vcpu-1gb\u0026#34;, VpcUuid = vpcId, PrivateNetworking = true, Region = \u0026#34;nyc3\u0026#34;, SshKeys = { sshKeyId }, UserData = userData }); }​ }​ There is a ton of power here and we are only stratching the surface. The big takeaways are that you shouldn\u0026rsquo;t be afraid of embracing multiple projects and stacks. Each stack is deployed via pulumi up independently, allowing you to focus on deploying only the infrastructure needed at a given time, reducing risk. It also enables a shared stack of resources you can use throughout your cloud solutions.\n","date":"2021-09-27T00:00:00Z","permalink":"https://shawn.vause.us/posts/think-outside-stack-pulumi/","title":"Think Outside the Stack With Pulumi"},{"content":"You may have noticed the blog went down recently. I have been working on a new design and re-launch that took a touch longer than anticipated. Thank you for returning. I am hoping to have a more robust solution available to write/publish content more easily now.  I intend to use this space to document things I have learned in my technical journeys and personal reading. Occasionally something non-technical might slip in, as I have used the pandemic to pursue new hobbies like fishing. While I may not be the most avid blogger, I think it will be a great reference for stuff I need to refer to periodically and my hope is that it will provide value to others as well. Thanks again for returning and I look forward to writing more content/iterating on this platform. Please bear with me for any bugs/defects that might arise as this is definitely a v1.0 platform at the moment. Welcome back and cheers! ","date":"2021-09-09T00:00:00Z","permalink":"https://shawn.vause.us/posts/welcome/","title":"Welcome Back!"}]